{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealTimeEmotionRecognizer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batuhanyndny/notebooks/blob/master/RealTimeEmotionRecognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ry6WYgWVJK4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0ac6a37e-e2b3-49a1-873c-04e4901cfe77"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/batuhanyndny/RealTimeEmotionRecognizer\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RealTimeEmotionRecognizer'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)   \u001b[K\rremote: Counting objects:  22% (2/9)   \u001b[K\rremote: Counting objects:  33% (3/9)   \u001b[K\rremote: Counting objects:  44% (4/9)   \u001b[K\rremote: Counting objects:  55% (5/9)   \u001b[K\rremote: Counting objects:  66% (6/9)   \u001b[K\rremote: Counting objects:  77% (7/9)   \u001b[K\rremote: Counting objects:  88% (8/9)   \u001b[K\rremote: Counting objects: 100% (9/9)   \u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)   \u001b[K\rremote: Compressing objects:  33% (2/6)   \u001b[K\rremote: Compressing objects:  50% (3/6)   \u001b[K\rremote: Compressing objects:  66% (4/6)   \u001b[K\rremote: Compressing objects:  83% (5/6)   \u001b[K\rremote: Compressing objects: 100% (6/6)   \u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 9 (delta 1), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  11% (1/9)   \rUnpacking objects:  22% (2/9)   \rUnpacking objects:  33% (3/9)   \rUnpacking objects:  44% (4/9)   \rUnpacking objects:  55% (5/9)   \rUnpacking objects:  66% (6/9)   \rUnpacking objects:  77% (7/9)   \rUnpacking objects:  88% (8/9)   \rUnpacking objects: 100% (9/9)   \rUnpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ht7as-m9gaTJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d49bfb3c-80e7-4fc4-9038-033d33f82288"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mRealTimeEmotionRecognizer\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxNIdPPxGd2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1387
        },
        "outputId": "2f65a2e6-7bcb-434e-8d79-8134ba81ee48"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "dataset_path = 'RealTimeEmotionRecognizer/dataset/fer2013/fer2013.csv'\n",
        "image_size=(48,48)\n",
        " \n",
        "def load_fer2013():\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    pixels = data['pixels'].tolist()\n",
        "    width, height = 48, 48\n",
        "    faces = []\n",
        "    for pixel_sequence in pixels:\n",
        "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "        face = np.asarray(face).reshape(width, height)\n",
        "        face = cv2.resize(face.astype('uint8'),image_size)\n",
        "        faces.append(face.astype('float32'))\n",
        "    faces = np.asarray(faces)\n",
        "    faces = np.expand_dims(faces, -1)\n",
        "    emotions = pd.get_dummies(data['emotion']).values\n",
        "    return faces, emotions\n",
        " \n",
        "def preprocess_input(x, v2=True):\n",
        "    x = x.astype('float32')\n",
        "    x = x / 255.0\n",
        "    if v2:\n",
        "        x = x - 0.5\n",
        "        x = x * 2.0\n",
        "    return x\n",
        " \n",
        "faces, emotions = load_fer2013()\n",
        "faces = preprocess_input(faces)\n",
        "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'pixels'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e0311feb1982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fer2013\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-e0311feb1982>\u001b[0m in \u001b[0;36mload_fer2013\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_fer2013\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'pixels'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ibxoLSUuGd2G",
        "outputId": "3a8d79e3-911f-4f20-c336-3153d217d8e2"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        " \n",
        "# parameters\n",
        "batch_size = 32\n",
        "num_epochs = 110\n",
        "input_shape = (48, 48, 1)\n",
        "verbose = 1\n",
        "num_classes = 7\n",
        "patience = 50\n",
        "base_path = 'models/'\n",
        "l2_regularization=0.01\n",
        " \n",
        "# data generator\n",
        "data_generator = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)\n",
        " \n",
        "# model parameters\n",
        "regularization = l2(l2_regularization)\n",
        " \n",
        "# base\n",
        "img_input = Input(input_shape)\n",
        "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        " \n",
        "# module 1\n",
        "residual = Conv2D(16, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "residual = BatchNormalization()(residual)\n",
        "x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "x = layers.add([x, residual])\n",
        " \n",
        "# module 2\n",
        "residual = Conv2D(32, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "residual = BatchNormalization()(residual)\n",
        "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "x = layers.add([x, residual])\n",
        " \n",
        "# module 3\n",
        "residual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
        "residual = BatchNormalization()(residual)\n",
        "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "x = layers.add([x, residual])\n",
        " \n",
        "# module 4\n",
        "residual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
        "residual = BatchNormalization()(residual)\n",
        "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "x = layers.add([x, residual])\n",
        "x = Conv2D(num_classes, (3, 3), padding='same')(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "output = Activation('softmax',name='predictions')(x)\n",
        " \n",
        "model = Model(img_input, output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n",
        " \n",
        "# callbacks\n",
        "log_file_path = base_path + '_emotion_training.log'\n",
        "csv_logger = CSVLogger(log_file_path, append=False)\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "trained_models_path = base_path + '_mini_XCEPTION'\n",
        "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
        "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        " \n",
        "model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n",
        "                        steps_per_epoch=len(xtrain) / batch_size,\n",
        "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
        "                        validation_data=(xtest,ytest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]            \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]            \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]            \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 58,423\n",
            "Trainable params: 56,951\n",
            "Non-trainable params: 1,472\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E8yufwM-Gd2L"
      },
      "cell_type": "code",
      "source": [
        "# FOR REAL TIME VIDEO\n",
        "\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import imutils\n",
        "import cv2\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# parameters for loading data and images\n",
        "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
        "emotion_model_path = 'models/_mini_XCEPTION.106-0.65.hdf5'\n",
        "\n",
        "# hyper-parameters for bounding boxes shape\n",
        "# loading models\n",
        "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
        "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
        "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
        " \"neutral\"]\n",
        "\n",
        "\n",
        "# starting video streaming\n",
        "cv2.namedWindow('your_face')\n",
        "camera = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    frame = camera.read()[1]\n",
        "    #reading the frame\n",
        "    frame = imutils.resize(frame,width=400)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    \n",
        "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
        "    frameClone = frame.copy()\n",
        "    if len(faces) > 0:\n",
        "        faces = sorted(faces, reverse=True,\n",
        "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
        "        (fX, fY, fW, fH) = faces\n",
        "                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 48x48 pixels, and then prepare\n",
        "            # the ROI for classification via the CNN\n",
        "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
        "        roi = cv2.resize(roi, (48, 48))\n",
        "        roi = roi.astype(\"float\") / 255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "        \n",
        "        \n",
        "        preds = emotion_classifier.predict(roi)[0]\n",
        "        emotion_probability = np.max(preds)\n",
        "        label = EMOTIONS[preds.argmax()]\n",
        "\n",
        " \n",
        "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
        "                # construct the label text\n",
        "                text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
        "                w = int(prob * 300)\n",
        "                cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
        "                (w, (i * 35) + 35), (0, 0, 255), -1)\n",
        "                cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
        "                (255, 255, 255), 2)\n",
        "                cv2.putText(frameClone, label, (fX, fY - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
        "                              (0, 0, 255), 2)\n",
        "\n",
        "    cv2.imshow('your_face', frameClone)\n",
        "    cv2.imshow(\"Probabilities\", canvas)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZRCZI3h-Gd2O"
      },
      "cell_type": "code",
      "source": [
        "# FOR IMAGE\n",
        "\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        "import imutils\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        " \n",
        "# parameters for loading data and images\n",
        "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
        "emotion_model_path = 'models/_mini_XCEPTION.106-0.65.hdf5'\n",
        "img_path = sys.argv[1]\n",
        " \n",
        "# hyper-parameters for bounding boxes shape\n",
        "# loading models\n",
        "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
        "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
        "EMOTIONS = [\"angry\",\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]\n",
        " \n",
        "#reading the frame\n",
        "orig_frame = cv2.imread(img_path)\n",
        "frame = cv2.imread(img_path,0)\n",
        "faces = face_detection.detectMultiScale(frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        " \n",
        "if len(faces) &amp;amp;amp;gt; 0:\n",
        "    faces = sorted(faces, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
        "    (fX, fY, fW, fH) = faces\n",
        "    roi = frame[fY:fY + fH, fX:fX + fW]\n",
        "    roi = cv2.resize(roi, (48, 48))\n",
        "    roi = roi.astype(\"float\") / 255.0\n",
        "    roi = img_to_array(roi)\n",
        "    roi = np.expand_dims(roi, axis=0)\n",
        "    preds = emotion_classifier.predict(roi)[0]\n",
        "    emotion_probability = np.max(preds)\n",
        "    label = EMOTIONS[preds.argmax()]\n",
        "    cv2.putText(orig_frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "    cv2.rectangle(orig_frame, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
        " \n",
        "cv2.imshow('test_face', orig_frame)\n",
        "cv2.imwrite('test_output/'+img_path.split('/')[-1],orig_frame)\n",
        "if (cv2.waitKey(2000) &amp;amp;amp;amp; 0xFF == ord('q')):\n",
        "    sys.exit(\"Thanks\")\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}